{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature - Interoperability with Official NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "import numpy as onp\n",
    "\n",
    "shape = (3, 4)\n",
    "a = np.random.uniform(size=shape)  # type of a is mxnet.numpy.ndarray\n",
    "b = np.random.uniform(size=shape)  # type of b is mxnet.numpy.ndarray\n",
    "print(\"type of a is\", type(a))\n",
    "print(\"type of b is\", type(b))\n",
    "c = onp.add(a, b)  # type of c is mxnet.numpy.ndarray\n",
    "print(\"type of c is\", type(c))\n",
    "\n",
    "c_sum = onp.sum(c, axis=0)  # type of c_sum is mxnet.numpy.ndarray\n",
    "print(\"type of c_sum is\", type(c_sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature - Parallel Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "lhs_shape = (4096, 1024)\n",
    "rhs_shape = (1024, 4096)\n",
    "\n",
    "onp_lhs = onp.random.uniform(-1.0, 1.0, size=lhs_shape).astype(np.float32)\n",
    "onp_rhs = onp.random.uniform(-1.0, 1.0, size=rhs_shape).astype(np.float32)\n",
    "\n",
    "onp_out = onp.dot(onp_lhs, onp_rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running with official NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "lhs_shape = (4096, 1024)\n",
    "rhs_shape = (1024, 4096)\n",
    "\n",
    "onp_lhs = onp.random.uniform(-1.0, 1.0, size=lhs_shape).astype(np.float32)\n",
    "onp_rhs = onp.random.uniform(-1.0, 1.0, size=rhs_shape).astype(np.float32)\n",
    "\n",
    "import time\n",
    "onp_start = time.time()\n",
    "onp_out = onp.dot(onp_lhs, onp_rhs)\n",
    "onp_end = time.time()\n",
    "print(\"official numpy consumed:\", onp_end - onp_start, \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running with NP on MXNet on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx, nd\n",
    "npx.set_np()\n",
    "\n",
    "lhs_shape = (4096, 1024)\n",
    "rhs_shape = (1024, 4096)\n",
    "\n",
    "\n",
    "dnp_cpu_lhs = np.random.uniform(-1.0, 1.0, size=lhs_shape).astype(np.float32)\n",
    "dnp_cpu_rhs = np.random.uniform(-1.0, 1.0, size=rhs_shape).astype(np.float32)\n",
    "\n",
    "import time\n",
    "dnp_cpu_start = time.time()\n",
    "dnp_cpu_out = np.dot(dnp_cpu_lhs, dnp_cpu_rhs)\n",
    "nd.waitall()\n",
    "dnp_cpu_end = time.time()\n",
    "print(\"NP on MXNet consumed:\", dnp_cpu_end - dnp_cpu_start, \"seconds on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running with NP on MXNet on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx, nd\n",
    "npx.set_np()\n",
    "\n",
    "lhs_shape = (4096, 1024)\n",
    "rhs_shape = (1024, 4096)\n",
    "\n",
    "if npx.num_gpus() >= 1:\n",
    "    dnp_gpu_lhs = np.random.uniform(-1.0, 1.0, size=lhs_shape).as_in_ctx(npx.gpu(0))\n",
    "    dnp_gpu_rhs = np.random.uniform(-1.0, 1.0, size=rhs_shape).as_in_ctx(npx.gpu(0))\n",
    "\n",
    "    import time\n",
    "    dnp_gpu_start = time.time()\n",
    "    dnp_gpu_out = np.dot(dnp_gpu_lhs, dnp_gpu_rhs)\n",
    "    nd.waitall()\n",
    "    dnp_gpu_end = time.time()\n",
    "    print(\"NP on MXNet consumed:\", dnp_gpu_end - dnp_gpu_start, \"seconds on GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "\n",
    "x = np.linspace(-4., 4.)\n",
    "sigmoid = npx.sigmoid(x)\n",
    "relu = npx.relu(x)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.1,1.0])\n",
    "plt.xlabel(\"x\"); plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, sigmoid, label='Sigmoid')\n",
    "plt.plot(x, relu, label='ReLU')\n",
    "axes.legend(fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization Function\n",
    "![BN_backward](https://kratzert.github.io/images/bn_backpass/BNcircuit.png)\n",
    "*figure credit: [Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "def batchnorm_forward(x, gamma, beta, eps=1e-5):\n",
    "    N = x.shape[0]\n",
    "    #step1: calculate mean\n",
    "    mu = onp.mean(x, axis = 0)\n",
    "    #step2: subtract mean vector of every trainings example\n",
    "    xmu = x - mu\n",
    "    #step3: following the lower branch - calculation denominator\n",
    "    sq = xmu ** 2\n",
    "    #step4: calculate variance\n",
    "    var = onp.mean(sq, axis = 0)\n",
    "    #step5: add eps for numerical stability, then sqrt\n",
    "    sqrtvar = onp.sqrt(var + eps)\n",
    "    #step6: invert sqrtwar\n",
    "    ivar = 1./sqrtvar\n",
    "    #step7: execute normalization\n",
    "    xhat = xmu * ivar\n",
    "    #step8: Nor the two transformation steps\n",
    "    gammax = gamma * xhat\n",
    "    #step9\n",
    "    out = gammax + beta\n",
    "    #store intermediate\n",
    "    cache = (xhat, gamma, xmu, ivar, sqrtvar, var, eps)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute with NP on MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "\n",
    "shape = (32, 784)\n",
    "\n",
    "batch_size = shape[0]\n",
    "\n",
    "data = np.random.uniform(-1.0, 1.0, size=shape)\n",
    "gamma = np.random.uniform(-1.0, 1.0, size=shape[1:])\n",
    "beta = np.random.uniform(-1.0, 1.0, size=shape[1:])\n",
    "dnp_out, dnp_cache = batchnorm_forward(data, gamma, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing with Official NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "onp_data = data.asnumpy()\n",
    "onp_gamma = gamma.asnumpy()\n",
    "onp_beta = beta.asnumpy()\n",
    "onp_out, onp_cache = batchnorm_forward(onp_data, onp_gamma, onp_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.test_utils import assert_almost_equal\n",
    "# assert_almost_equal is a mxnet helper function\n",
    "# that is similar to numpy.testing.assert_almost_equal,\n",
    "# it will raise error if the 2 inputs do not match\n",
    "try:\n",
    "    assert_almost_equal(dnp_out, onp_out, atol=1e-5, rtol=1e-3)\n",
    "    for dnp_arr, onp_arr in zip(dnp_cache, onp_cache):\n",
    "        assert_almost_equal(dnp_arr, onp_arr, atol=1e-5, rtol=1e-3)\n",
    "    print(\"the outputs match!\")\n",
    "except:\n",
    "    print(\"the outputs do not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backward Propagation\n",
    "![BN_backward](https://kratzert.github.io/images/bn_backpass/BNcircuit.png)\n",
    "*figure credit: [Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing Gradient with Official NumPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "def batchnorm_backward(dout, cache):\n",
    "    # requires the intermediate values for faster computations\n",
    "    xhat, gamma, xmu, ivar, sqrtvar, var, eps = cache\n",
    "    N = dout.shape[0]  # get the batch size\n",
    "\n",
    "    dbeta = onp.sum(dout, axis=0)  # step 9\n",
    "    dgammax = dout  # not necessary, but more understandable\n",
    "\n",
    "    dgamma = onp.sum(dgammax*xhat, axis=0)  #step 8\n",
    "    dxhat = dgammax * gamma  # step 8\n",
    "\n",
    "    divar = onp.sum(dxhat*xmu, axis=0)  # step 7\n",
    "    dxmu1 = dxhat * ivar  # step 7\n",
    "\n",
    "    dsqrtvar = -divar / (sqrtvar**2)  # step 6\n",
    "\n",
    "    dvar = 0.5 / onp.sqrt(var + eps) * dsqrtvar  # step5\n",
    "    \n",
    "    dsq = onp.ones(dout.shape, dtype=onp.float32) * dvar / N  # step 4\n",
    "    \n",
    "    dxmu2 = 2 * xmu * dsq  # step 3\n",
    "\n",
    "    dx1 = dxmu1 + dxmu2  # step 2\n",
    "    dmu = -onp.sum(dx1, axis=0)  # step 2\n",
    "\n",
    "    dx2 = onp.ones(dout.shape, dtype=onp.float32) * dmu / N  # step 1\n",
    "\n",
    "    dx = dx1 + dx2  # step 0\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "dout = np.random.uniform(size=onp_out.shape)\n",
    "\n",
    "onp_dout = dout.asnumpy()\n",
    "# forward pass\n",
    "onp_out, onp_cache = batchnorm_forward(onp_data, onp_gamma, onp_beta)\n",
    "# backward pass\n",
    "onp_dx, onp_dgamma, onp_dbeta = batchnorm_backward(onp_dout, onp_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing Gradient with Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "# attach gradient buffers to the arrays we want to compute\n",
    "# gradient for\n",
    "data.attach_grad()\n",
    "gamma.attach_grad()\n",
    "beta.attach_grad()\n",
    "\n",
    "with autograd.record():  # autograd.record() gives a scope which captures\n",
    "                         # code that needs gradient computation\n",
    "    dnp_out, dnp_cache = batchnorm_forward(data, gamma, beta)\n",
    "\n",
    "dnp_out.backward(out_grad=dout)  # compute the gradients with respect to dnp_out's\n",
    "                                 # variables. In this case, dnp_out is computed by\n",
    "                                 # batchnorm_forward(data, gamma, beta)\n",
    "                                 # so data, gamma and beta are out's variables\n",
    "\n",
    "# data's gradient now in data.grad\n",
    "# gamma's gradient now in gamma.grad\n",
    "# beta's gradient now in beta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.test_utils import assert_almost_equal\n",
    "# assert_almost_equal is a mxnet helper function\n",
    "# that is similar to numpy.testing.assert_almost_equal,\n",
    "# it will raise error if the 2 inputs do not match\n",
    "\n",
    "try:\n",
    "    assert_almost_equal(data.grad, onp_dx, atol=1e-5, rtol=1e-3)\n",
    "    assert_almost_equal(gamma.grad, onp_dgamma, atol=1e-5, rtol=1e-3)\n",
    "    assert_almost_equal(beta.grad, onp_dbeta, atol=1e-5, rtol=1e-3)\n",
    "    print(\"the gradients match!\")\n",
    "except:\n",
    "    print(\"the gradients do not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a model with NP on MXNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting the Environment Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# import np and npx modules\n",
    "from mxnet import np, npx\n",
    "# setting MXNet to NumPy-compatible mode\n",
    "npx.set_np()\n",
    "# we'll be using automatic differentiation\n",
    "from mxnet import autograd\n",
    "# borrowing the dataloader from our Gluon library\n",
    "from mxnet import gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prepare the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256  # setting the batch size\n",
    "def load_data_fashion_mnist(batch_size):\n",
    "    dataset = gluon.data.vision\n",
    "    trans = [dataset.transforms.ToTensor()]\n",
    "    trans = dataset.transforms.Compose(trans)\n",
    "    mnist_train = dataset.FashionMNIST(train=True).transform_first(trans)\n",
    "    mnist_test = dataset.FashionMNIST(train=False).transform_first(trans)\n",
    "    return (gluon.data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                                  num_workers=4),\n",
    "            gluon.data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "                                  num_workers=4))\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)\n",
    "# train_iter and test_iter are dataset iterators\n",
    "# for training set and test set respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initializing all Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# a 2-layer multi-layer perceptron with batch normalization\n",
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "# layer 1 weight\n",
    "W1 = np.random.normal(scale=0.01, size=(num_inputs, num_hiddens))\n",
    "# layer 1 bias\n",
    "b1 = np.zeros(num_hiddens)\n",
    "# gamma parameter for batch normalization\n",
    "gamma = np.ones(shape=(num_hiddens,))\n",
    "# beta parameter for batch normalization\n",
    "beta = np.zeros(shape=(num_hiddens,))\n",
    "# layer 2 weight\n",
    "W2 = np.random.normal(scale=0.01, size=(num_hiddens, num_outputs))\n",
    "# layer 2 bias\n",
    "b2 = np.zeros(num_outputs)\n",
    "# collect all parameter for easier access\n",
    "params = [W1, b1, gamma, beta, W2, b2]\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()  # attach gradient to all parameters\n",
    "                         # for using automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    # ReLU activation function: f(x) = x if x > 0 else 0\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def batchnorm(x, gamma, beta, eps=1e-5):\n",
    "    N = x.shape[0]\n",
    "\n",
    "    mu = np.mean(x, axis = 0)\n",
    "    xmu = x - mu\n",
    "\n",
    "    sq = xmu ** 2\n",
    "    var = np.mean(sq, axis = 0)\n",
    "    sqrtvar = np.sqrt(var + eps)\n",
    "    ivar = 1. / sqrtvar\n",
    "\n",
    "    xhat = xmu * ivar\n",
    "\n",
    "    gammax = gamma * xhat\n",
    "    out = gammax + beta\n",
    "\n",
    "    return out\n",
    "\n",
    "def net(X):\n",
    "    X = X.reshape(-1, num_inputs)  # (batch_size, 28, 28) -> (batch_size, 784)\n",
    "    H = relu(np.dot(X, W1) + b1)   # H: (batch_size, 256)\n",
    "    H_normed = batchnorm(H, gamma, beta) # H_normed: (batch_size, 256)\n",
    "    return np.dot(H_normed, W2) + b2     # output: (batch_size, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setting up Loss and Optimizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(y_hat):\n",
    "    # stable version of softmax function\n",
    "    exps = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def loss(y_hat, y):\n",
    "    m = y.shape[0]\n",
    "    p = softmax(y_hat)\n",
    "    # cross entropy = -sum(y_i * log(p(y_i)))\n",
    "    return np.sum(-np.log(p[range(m), y]))\n",
    "\n",
    "# stochastic gradient descent\n",
    "def sgd(params, lr, wd, batch_size):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad / batch_size - param * wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    # y_hat: (batch_size, 10)\n",
    "    # y:     (batch_size, 1)\n",
    "    # label prediction of the model is the index\n",
    "    # of the maximum value for each sample\n",
    "    return float((y_hat.argmax(axis=1) == y.astype('float32')).sum())\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    num_correct_example = 0\n",
    "    total_num_example = 0\n",
    "\n",
    "    for X, y in data_iter:\n",
    "        num_correct_example += accuracy(net(X), y)\n",
    "        total_num_example += y.size\n",
    "    # return the percentage of correct predictions\n",
    "    return num_correct_example / total_num_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 10  # training for 10 epochs\n",
    "lr = 0.5         # with learning rate of 0.5\n",
    "wd = 0.001       # with weight decay of 0.001\n",
    "\n",
    "for epoch in range(num_epochs):  # for each epoch\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0\n",
    "    num_examples = 0\n",
    "    for X, y in train_iter:\n",
    "        with autograd.record():\n",
    "            y_hat = net(X)       # forward pass\n",
    "            l = loss(y_hat, y)   # computing loss\n",
    "        l.backward()\n",
    "        # at this moment the gradients are ready in the\n",
    "        # gradient buffers of all parameters in params\n",
    "        sgd(params, lr, wd, X.shape[0])  # update parameters\n",
    "        train_loss += l.sum()            # accumulate loss\n",
    "        train_acc += accuracy(y_hat, y)  # accumulate accuracy\n",
    "        num_examples += y.size           # count total number of samples\n",
    "    # Return training loss and training accuracy\n",
    "    train_loss, train_acc = train_loss/num_examples, train_acc/num_examples\n",
    "    test_acc = evaluate_accuracy(net, test_iter)\n",
    "    print(\"epoch {}: train loss {} \"\n",
    "          \"train accuracy {} test accuracy {}\".format(epoch,\n",
    "                                                      train_loss,\n",
    "                                                      train_acc,\n",
    "                                                      test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
